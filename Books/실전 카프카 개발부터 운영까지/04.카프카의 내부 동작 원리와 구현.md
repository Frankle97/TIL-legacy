# 카프카의 내부 동작 원리와 구현
## 카프카 리플리케이션
카프카는 초기 설계부터 하드웨어 이슈 등으로 브로커에서 장애가 발생해도 데이터 허브로써 안정적인 서비스가 운영될 수 있도록 설계됐다.  
안정성을 확보하기 위해 카프카 내부에서는 리플리케이션이란 동작을 한다.

### 리플리케이션 동작 개요
카프카 리플리케이션 동작을 위해 토픽 생성 시 필수값으로 `replication factor` 옵션을 설정해주어야 한다.

### 리더와 팔로워
카프카는 내부적으로 모두 동일한 리플리케이션들을 리더와 팔로워로 구분하고, 각자 역할을 분담시킨다.  
리더는 리플리케이션 중 하나가 선정되며, 모든 읽기 쓰기는 리더를 통해서만 가능하다.  
즉, 프로듀서는 모든 리플리케이션에 메시지를 보내는 것이 아닌 리더에게만 메시지를 전송한다.  
컨슈머 또한 리더로부터만 메시지를 가져온다.

리더는 메시지를 읽고 쓰는 동작을 하고, 팔로워들은 리더에 문제가 발생할 경우 언제든 새로운 리더가 될 준비를 해야 한다.  
따라서 팔로워는 리더가 새로운 메시지를 받았는지 확인하고, 메시지가 있다면 리더로부터 복제한다.

### 복제 유지와 커밋
- 리더와 팔로위는 ISF(InSyncReplica)라는 논리적 그룹으로 묶여있다.
- 해당 그룹에 속한 팔로워들만이 새로운 리더의 자격을 가질 수 있다.
- 팔로워들은 데이터 일치를 유지하기 위해 리더의 데이터를 따라가고, 리더는 팔로워들이 전부 메시지를 받을 때까지 기다린다.
- 만약 어떠한 원인 때문에 리더로부터 리플리케이션 하지 못할 경우, 그리고 이 문제가 생겼던 팔로워에게 리더를 넘기게 된다면 데이터 정합성 등의 문제가 발생할 수 있다.
  - 따라서 리더는 팔로워들이 원활하게 리플리케이션 하고 있는지를 감시한다.
    - 특정 주기만큼 복제 요청을 하지 않으면, 문제가 발생했다고 판단해 해당 팔로워를 그룹에서 제외시킨다.
- 정상 리플리케이션을 하고 있는 팔로워들만이 그룹에 속하며, 리더에 문제가 있으면 새로운 리더 자격을 얻을 수 있다.


- ISR 내에 모든 팔로워가 복제를 마쳤다면 리더는 내부적으로 커밋 표시를 한다.
  - 마지막 커밋 오프셋의 위치를 **하이워터마크**라고 한다.
- 리더가 커밋을 했다는 것은 리플리케이션 팩터들이 모두 메시지를 저장했다는 뜻이다.
- 컨슈머는 메시지의 일관성을 위해 **커밋된 메시지만 읽어갈 수 있다.**
- 마지막 커밋의 오프셋 위치는 `/data/kafka-logs/replication-offset-checkpoint` 파일에 있다.
  - `[토픽명] [파티션 번호] [커밋된 오프셋 번호]`로 이루어져 있다.

### 리더와 팔로워의 단계별 리플리케이션 동작
- 카프카로 향하는 수많은 메시지를 읽고 쓰기를 처리하는 리더는 매우 바쁘다.
  - 바쁜 리더가 리플리케이션 동작을 위해 팔로워들과 많은 통신을 주고 받거나 많은 관여를 한다면 리더 성능이 떨어짐과 동시에 카프카 성능도 떨어지게 되는 것이다.
- 따라서 카프카는 리더와 팔로워 간의 리플리케이션 동작을 처리할 때 통신을 최소화할 수 있도록 설계했다.
- 카프카는 리더와 팔로워 사이의 **ACK 통신을 제거**함으로써 리플리케이션 동작 성능을 높였다.
- 리플리케이션은 리더가 Push 하는 것이 아닌, **팔로워들이 Pull** 하는 방식으로 리더의 부하를 줄였다. 


### 리더에포크와 복구
- 리더에포크(*LeaderEpoch*)는 **파티션들이 복구 동작을 할 때 메시지 일관성을 유지하기 위한 용도**로 이용된다.
- 복구 작업에서 팔로워는 갖고 있는 메시지 중 자신의 워터마크보다 높은 메시지는 신뢰할 수 없으므로 삭제하고, 재요청한다. 
- 